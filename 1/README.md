## Answers
### 1. 1. The data extraction depends on format of raw data (most likely it will be a binary data from stream). For the data to be conventional for our system I would parse the data into JSON (With game identifiers, time intervals, etc.). The size of JSON should not be a big problem as I expect data to not be cumulative. The main problem I think may be with parsing data from many concurrent games as it may cause some overloads inside the system. Probably there should be implemented some sort of scaling for this gateway. After the parse of JSON the data also will be sent to:
* Lightweight API where it is separated into the most valuable data which will be available to end users through Pub/Sub mechanism. I moved the separation of data into separate gateway/service to not overload parsing gateway even more.  
* Statistics service where based on identifiers and time intervals data will be saved into DB in form of statistics and will be available to users through RESTful api. (Both close to real time and after game). This statistics will be updated every 1 minute (to not overload the service), even though the data updates may come faster from parsing gateway (most likely new JSON data will be stored in queue).

### 1. 2. The mechanism would be very simple as the Parsing gateway will parse the raw data and publish it for the end users to consume as well as send it to statistics service. The main challenge as I said previously may be the format of the raw data and the amount of concurrent games. Also the complexity of parsing may become an issue in terms of resources.

### 1. 3. The most useful patterns here are EventEmitter and Pub/Sub. They may be a good choice for data publishing for end users as well as emitting the moment when the data is accepted from external system. (E.g. we emit the parsing mechanism after we receive all chunks of raw data for certain time period). As for the rest of the system, we can use straightforward client-server pattern.